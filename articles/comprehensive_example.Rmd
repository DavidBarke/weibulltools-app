---
title: "Comprehensive Example"
author: 
  - "Tim-Gunnar Hensel"
  - "David Barkemeyer"
date: "16 2 2021"
output: 
  html_document
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  screenshot.force = FALSE,
  comment = "#>"
)
library(weibulltools)
```

This document presents non-parametric estimation methods for the computation of 
failure probabilities of complete data (failures) taking (multiple) right-censored 
units into account. A unit can be either a single component, an assembly or an entire system.  

Further, the estimation results are presented in distribution-specific probability plots.

## Introduction to Life Data Analysis

If the lifetime (or any other damage-equivalent quantity such as distance or 
load cycles) of a unit is considered to be a continuous random variable _T_, 
then the probability that a unit has failed at _t_ is defined by its 
_CDF (cumulative distribution function)_ _F(t)_.

$$ P(T\leq t) = F(t) $$

In order to obtain an estimate of the _CDF_ for each observation $t_1, t_2, ..., t_n$ 
two approaches are possible. Using a parametric lifetime distribution requires that 
the underlying assumptions for the sample data are valid. If the distribution-specific 
assumptions are correct, the model parameters can be estimated and the _CDF_ is 
computable. But if assumptions are not held, interpretations and derived conclusions 
are not reliable.

A more general approach for the calculation of cumulative failure probabilities is 
to use non-parametric statistical estimators $\hat{F}(t_1), \hat{F}(t_2), ..., \hat{F}(t_n)$. 
In comparison to a parametric distribution no general assumptions must be held. 
For non-parametric estimators, an ordered sample of size $n$ is needed. Starting 
at $1$, the ranks $i \in \{1, 2, ..., n \}$ are assigned to the ascending sorted 
sample values. Since there is a known relationship between ranks and corresponding 
ranking probabilities a _CDF_ can be determined.  

But rank distributions are systematically skewed distributions and thus the median 
value instead of the expected value $E\left[F\left(t_i\right)\right] = \frac{i}{n + 1}$ 
is used for the estimation [^note1]. This skewness is visualized in _Figure 1_. 

[^note1]: Kapur, K. C.; Lamberson, L. R.: _Reliability in Engineering Design_, 
          _New York: Wiley_, 1977, pp. 297-301  

```{r rank_densities, fig.cap = "Figure 1: Densities for different ranks i in samples of size n = 10.", message = FALSE, warning = FALSE}
library(dplyr) # data manipulation 
library(ggplot2) # visualization

x <- seq(0, 1, length.out = 100) # CDF
n <- 10 # sample size
i <- c(1, 3, 5, 7, 9) # ranks
r <- n - i + 1 # inverse ranking

df_dens <- expand.grid(cdf = x, i = i) %>% 
  mutate(n = n, r = n - i + 1, pdf = dbeta(x = x, shape1 = i, shape2 = r))

densplot <- ggplot(data = df_dens, aes(x = cdf, y = pdf, colour = as.factor(i))) + 
  geom_line() + 
  scale_colour_discrete(guide = guide_legend(title = "i")) + 
  theme_bw() + 
  labs(x = "Failure Probability", y = "Density")
densplot
```

### Failure Probability Estimation  

In practice, a simplification for the calculation of the median value, also called 
median rank, is made. The formula of _Benard's_ approximation is given by 

$$\hat{F}(t_i) \approx \frac{i - 0,3}{n + 0,4} $$ 

and is described in _The Plotting of Observations on Probability Paper_ [^note2]. 

[^note2]: Benard, A.; Bos-Levenbach, E. C.: _The Plotting of Observations on Probability Paper_, 
          _Statistica Neerlandica 7 (3)_, 1953, pp. 163-173  
          
However, this equation only provides valid estimates for failure probabilities if 
all units in the sample are defectives (`estimate_cdf(methods = "mr", ...)`).

In field data analysis, however, the sample mainly consists of intact units and 
only a small fraction of units failed. Units that have no damage at the point of 
analysis and also have not reached the operating time or mileage of units that 
have already failed, are potential candidates for future failures.  As these, 
for example, still are likely to fail during a specific time span, like the 
guarantee period, the _CDF_ must be adjusted upwards by these potential candidates.  

A commonly used method for correcting probabilities of (multiple) right-censored 
data is _Johnson's_ method (`estimate_cdf(methods = "johnson", ...)`). By this method, 
all units that are included in the period looked at are sorted in an ascending order 
of their operating time (or any other damage-equivalent quantity). If there are units 
that have not failed before the _i_-th failure, an adjusted rank for the _i_-th failure 
is formed. This correction takes the potential candidates into account and increases 
the rank number. In consequence, a higher rank leads to a higher failure probability. 
This can be seen in _Figure 1_.
  
The rank adjustment is determined with: 

$$j_i = j_{i-1} + x_i \cdot I_i, \;\; with \;\; j_0 = 0$$

Here, $j_ {i-1}$ is the adjusted rank of the previous failure, $x_i$ is the number 
of defectives at $t_i$ and $I_i$ is the increment that corrects the considered rank 
by the potential candidates. 

$$I_i=\frac{(n+1)-j_{i-1}}{1+(n-n_i)}$$

The sample size is $n$ and $n_i$ is the number of units that have a lower $t$ than 
the _i_-th unit. Once the adjusted ranks are calculated, the _CDF_ can be estimated 
according to _Benard's_ approximation.  

Other methods in `weibulltools` that can also handle (multiple) right-censored data 
are the _Kaplan-Meier_ estimator (`estimate_cdf(methods = "kaplan", ...)`) and the 
_Nelson-Aalen_ estimator (`estimate_cdf(methods = "nelson", ...)`). 

### Probability Plotting  

After computing failure probabilities a method called _probability plotting_ is 
applicable. It is a graphical _goodness of fit_ technique that is used in assessing 
whether an assumed distribution is appropriate to model the sample data.  

The axes of a probability plot are transformed in such a way that the _CDF_ of a 
specified model is represented through a straight line. If the plotted points 
(`plot_prob()`) lie on an approximately straight line it can be said that the 
chosen distribution is adequate.  

The two-parameter Weibull distribution can be parameterized with parameters $\mu$ 
and $\sigma$ such that the _CDF_ is characterized by the following equation:  

$$F(t)=\Phi_{SEV}\left(\frac{\log(t) - \mu}{\sigma}\right)$$

The advantage of this representation is that the Weibull is part of the 
(log-)location-scale family. A linearized representation of this _CDF_ is: 

$$\Phi^{-1}_{SEV}\left[F(t)\right]=\frac{1}{\sigma} \cdot \log(t) - \frac{\mu}{\sigma}$$

This leads to the following transformations regarding the axes: 

* Abscissa: $x = \log(t)$ 
* Ordinate: $y = \Phi^{-1}_{SEV}\left[F(t)\right]$, which is the quantile function 
  of the SEV (_smallest extreme value_) distribution and can be written out with 
  $\log\left\{-\log\left[1-F(t)\right]\right\}$.  

Another version of the Weibull _CDF_ with parameters $\eta$ and $\beta$ results in 
a _CDF_ that is defined by the following equation:  

$$F(t)=1-\exp\left[ -\left(\frac{t}{\eta}\right)^{\beta}\right]$$

Then a linearized version of the CDF is: 

$$ \log\left\{-\log\left[1-F(t)\right]\right\} = \beta \cdot \log(t) - \beta \cdot \log(\eta)$$

Transformations regarding the axes are: 

* Abscissa: $x = \log(t)$ 
* Ordinate: $y = \log\left\{-\log\left[1-F(t)\right]\right\}$.
  
It can be easily seen that the parameters can be converted into each other. The 
corresponding equations are: 

$$\beta = \frac{1}{\sigma}$$  

and 

$$\eta = \exp\left(\mu\right).$$   

## Data: Shock Absorber

To apply the introduced methods of non-parametric failure probability estimation 
and probability plotting the `shock` data is used. In this dataset kilometer-dependent 
problems that have occurred on shock absorbers are reported. In addition to failed 
items the dataset also contains non-defectives (*censored*) observations. The data 
can be found in _Statistical Methods for Reliability Data_ [^note3]. 

[^note3]: Meeker, W. Q.; Escobar, L. A.: _Statistical Methods for Reliability Data_, 
          _New York, Wiley series in probability and statistics_, 1998, p. 630
          
For consistent handling of the data, `weibulltools` introduces the function 
`reliability_data()` that converts the original dataset into a `wt_reliability_data` 
object. This formatted object allows to easily apply the presented methods.  

```{r dataset_shock, message = FALSE}
shock_tbl <- reliability_data(data = shock, x = distance, status = status)
shock_tbl
```

## Estimation of Failure Probabilities with Package `weibulltools`

First, we are interested in how censored observations influence the estimation of 
failure probabilities in comparison to the case where only failed units are considered. 
To deal with survived and failed units we will use `estimate_cdf()` with 
`methods = "johnson"`, whereas `methods = "mr"` only considers failures. 

```{r failure_probabilities}
# Estimate CDF with both methods: 
cdf_tbl <- estimate_cdf(shock_tbl, methods = c("mr", "johnson"))

# First case where only failed units are taken into account:
cdf_tbl_mr <- cdf_tbl %>% filter(cdf_estimation_method == "mr")
cdf_tbl_mr

# Second case where both, survived and failed units are considered:
cdf_tbl_john <- cdf_tbl %>% filter(cdf_estimation_method == "johnson") 
cdf_tbl_john
```

<br>

If we compare both outputs we can see that survivors reduce the probabilities. 
But this is just that what was expected since undamaged units with longer or equal 
lifetime characteristic _x_ let us gain confidence in the product. 

## Probability Plotting with Package `weibulltools`

The estimated probabilities should now be presented in a probability plot. 
With `plot_prob()` probability plots for several lifetime distributions can be constructed 
and estimates of multiple methods can be displayed at once. 

### Weibull Probability Plot

```{r probability_plot_weibull_2, fig.cap = "Figure 3: Plotting positions in Weibull grid.", message = FALSE}
# Weibull grid for estimated probabilities: 
weibull_grid <- plot_prob(
  cdf_tbl,
  distribution = "weibull", 
  title_main = "Weibull Probability Plot", 
  title_x = "Mileage in km", 
  title_y = "Probability of Failure in %",
  title_trace = "Method",
  plot_method = "ggplot2"
)

weibull_grid
```

<br>

_Figure 3_ shows that the consideration of survivors (orange points, _Method: johnson_) 
decreases the failure probability in comparison to the sole evaluation of failed 
items (green points, _Method: mr_).  

### Log-normal Probability Plot

Finally, we want to use a log-normal probability plot to visualize the estimated 
failure probabilities.

```{r probability_plot_log-normal, fig.cap = "Figure 4: Plotting positions in log-normal grid.", message = FALSE}
# Log-normal grid for estimated probabilities: 
lognorm_grid <- plot_prob(
  cdf_tbl,
  distribution = "lognormal",
  title_main = "Log-normal Probability Plot",
  title_x = "Mileage in km",
  title_y = "Probability of Failure in %",
  title_trace = "Method",
  plot_method = "ggplot2"
)

lognorm_grid
```

<br>

On the basis of _Figure 3_ and _Figure 4_ we can subjectively assess the goodness 
of fit of Weibull and log-normal. It can be seen that in both grids, the plotted 
points roughly fall on a straight line. Hence one can say that the Weibull as well 
as the log-normal are good model candidates for the `shock` data.

---

This document introduces two methods for the parameter estimation of lifetime 
distributions. Whereas _Rank Regression (RR)_ fits a straight line through 
transformed plotting positions (transformation is described precisely in 
`vignette(topic = "Life_Data_Analysis_Part_I", package = "weibulltools")`), 
_Maximum likelihood (ML)_ strives to maximize a function of the parameters given 
the sample data. If the parameters are obtained, a cumulative distribution function 
_(CDF)_ can be computed and added to a probability plot.  

In the theoretical part of this vignette the focus is on the two-parameter Weibull 
distribution. The second part is about the application of the provided estimation 
methods in `weibulltools`. All implemented models can be found in the help pages 
of `rank_regression()` and `ml_estimation()`.

## The Weibull Distribution 

The Weibull distribution is a continuous probability distribution, which is 
specified by the location parameter $\mu$ and the scale parameter $\sigma$. Its _CDF_ 
and _PDF (probability density function)_ are given by the following formulas: 

$$F(t)=\Phi_{SEV}\left(\frac{\log(t) - \mu}{\sigma}\right)$$

$$f(t)=\frac{1}{\sigma t}\;\phi_{SEV}\left(\frac{\log(t) - \mu}{\sigma}\right)$$
The practical benefit of the Weibull in the field of lifetime analysis is that 
the common profiles of failure rates, which are observed over the lifetime of a 
large number of technical products, can be described using this statistical distribution.

In the following, the estimation of the specific parameters $\mu$ and $\sigma$ is explained.  

## Rank Regression (RR) 

In _RR_ the _CDF_ is linearized such that the true, unknown population is estimated 
by a straight line which is analytically placed among the plotting pairs. The 
lifetime characteristic, entered on the x-axis, is displayed on a logarithmic scale. 
A double-logarithmic representation of the estimated failure probabilities is used 
for the y-axis. Ordinary Least Squares _(OLS)_ determines a best-fit line in order 
that the sum of squared deviations between this fitted regression line and the 
plotted points is minimized.  

In reliability analysis, it became prevalent that the line is placed in the probability 
plot in the way that the horizontal distances between the best-fit line and the 
points are minimized [^note1]. This procedure is called __x on y__ rank regression.  

[^note1]: Berkson, J.: _Are There Two Regressions?_, 
          _Journal of the American Statistical Association 45 (250)_, 
          DOI: 10.2307/2280676, 1950, pp. 164-180  
          
The formulas for estimating the slope and the intercept of the regression line 
according to the described method are given below.  

Slope: 

$$\hat{b}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})\cdot(y_i-\bar{y})}{\sum_{i=1}^{n}(y_i-\bar{y})^2}$$  

Intercept:  

$$\hat{a}=\bar{x}-\hat{b}\cdot\bar{y}$$  

With  

$$x_i=\log(t_i)\;;\; \bar{x}=\frac{1}{n}\cdot\sum_{i=1}^{n}\log(t_i)\;;$$  

as well as  

$$y_i=\Phi^{-1}_{SEV}\left[F(t)\right]=\log\left\{-\log\left[1-F(t_i)\right]\right\}\;and \; \bar{y}=\frac{1}{n}\cdot\sum_{i=1}^{n}\log\left\{-\log\left[1-F(t_i)\right]\right\}.$$  
The estimates of the intercept and slope are equal to the Weibull parameters $\mu$ and 
$\sigma$, i.e. 

$$\hat{\mu}=\hat{a}$$

and 

$$\hat{\sigma}=\hat{b}.$$

In order to obtain the parameters of the shape-scale parameterization the intercept 
and the slope need to be transformed [^note2].  

[^note2]: ReliaSoft Corporation: _Life Data Analysis Reference Book_, 
          online: [ReliaSoft](http://reliawiki.org/index.php/The_Weibull_Distribution), accessed 19 December 2020  

$$\hat{\eta}=\exp(\hat{a})=\exp(\hat{\mu})$$

and 

$$\hat{\beta}=\frac{1}{\hat{b}}=\frac{1}{\hat{\sigma}}.$$  

## Maximum Likelihood (ML)

The _ML_ method of Ronald A. Fisher estimates the parameters by maximizing the 
likelihood function. Assuming a theoretical distribution, the idea of _ML_ is that 
the specific parameters are chosen in such a way that the plausibility of obtaining 
the present sample is maximized. The likelihood and log-likelihood are given by the following equations:  

$$L = \prod_{i=1}^n\left\{\frac{1}{\sigma t_i}\;\phi_{SEV}\left(\frac{\log(t_i) - \mu}{\sigma}\right)\right\}$$ 

and 

$$\log L = \sum_{i=1}^n\log\left\{\frac{1}{\sigma t_i}\;\phi_{SEV}\left(\frac{\log(t_i) - \mu}{\sigma}\right)\right\}$$  

Deriving and nullifying the log-likelihood function according to parameters results in two 
formulas that have to be solved numerically in order to obtain the estimates.  

In large samples, ML estimators have optimality properties. In addition, the 
simulation studies by _Genschel and Meeker_ [^note3] have shown that even in small 
samples it is difficult to find an estimator that regularly has better properties 
than ML estimators.

[^note3]: Genschel, U.; Meeker, W. Q.: _A Comparison of Maximum Likelihood and Median-Rank Regression for Weibull Estimation_, 
          in: _Quality Engineering 22 (4)_, DOI: 10.1080/08982112.2010.503447, 2010, pp. 236-255


## Data

To apply the introduced parameter estimation methods the `shock` and `alloy` 
datasets are used. 

### Shock Absorber

In this dataset kilometer-dependent problems that have occurred on shock absorbers 
are reported. In addition to failed items the dataset also contains non-defectives 
(*censored*) observations. The data can be found in _Statistical Methods for Reliability Data_ [^note4]. 

[^note4]: Meeker, W. Q.; Escobar, L. A.: _Statistical Methods for Reliability Data_, 
          _New York, Wiley series in probability and statistics_, 1998, p. 630  
          
For consistent handling of the data, `weibulltools` introduces the function `reliability_data()` 
that converts the original dataset into a `wt_reliability_data` object. This formatted object 
allows to easily apply the presented methods.  

```{r dataset_shock_2, message = FALSE}
shock_tbl <- reliability_data(data = shock, x = distance, status = status)
shock_tbl
```          

### Alloy T7989

The dataset `alloy` in which the cycles until a fatigue failure of a 
special alloy occurs are inspected. The data is also taken from Meeker and Escobar [^note5]. 

[^note5]: Meeker, W. Q.; Escobar, L. A.: _Statistical Methods for Reliability Data_, 
          _New York, Wiley series in probability and statistics_, 1998, p. 131  
          
Again, the data have to be formatted as a `wt_reliability_data` object: 

```{r, data_alloy}
# Data: 
alloy_tbl <- reliability_data(data = alloy, x = cycles, status = status)
alloy_tbl
```

## RR and ML with Package `weibulltools`

`rank_regression()` and `ml_estimation()` can be applied to complete data as well 
as failure and (multiple) right-censored data. Both methods can also deal with models 
that have a threshold parameter $\gamma$.  

In the following both methods are applied to the dataset `shock`.

### RR for two-parameter Weibull distribution

```{r RR_weibull, fig.cap = "Figure 1: RR for a two-parametric Weibull distribution.", message = FALSE}
# rank_regression needs estimated failure probabilities: 
shock_cdf <- estimate_cdf(shock_tbl, methods = "johnson")

# Estimating two-parameter Weibull: 
rr_weibull <- rank_regression(shock_cdf, distribution = "weibull")
rr_weibull 

# Probability plot: 
weibull_grid <- plot_prob(
  shock_cdf,
  distribution = "weibull", 
  title_main = "Weibull Probability Plot", 
  title_x = "Mileage in km", 
  title_y = "Probability of Failure in %",
  title_trace = "Defectives",
  plot_method = "ggplot2"
)

# Add regression line: 
weibull_plot <- plot_mod(
  weibull_grid,
  x = rr_weibull,
  title_trace = "Rank Regression"
)

weibull_plot
```

### ML for two-parameter Weibull distribution

```{r ML_weibull, fig.cap = "Figure 2: ML for a two-parametric Weibull distribution.", message = FALSE}
# Again estimating Weibull: 
ml_weibull <- ml_estimation(
  shock_tbl, 
  distribution = "weibull"
)

ml_weibull 

# Add ML estimation to weibull_grid: 
weibull_plot2 <- plot_mod(
  weibull_grid, 
  x = ml_weibull, 
  title_trace = "Maximum Likelihood"
)

weibull_plot2
```

### ML for two- and three-parameter log-normal distribution

Finally, two- and three-parametric log-normal distributions are fitted to the 
`alloy` data using maximum likelihood.  

```{r ML_estimation_log-normal, message = FALSE}
# Two-parameter log-normal:  
ml_lognormal <- ml_estimation(
  alloy_tbl,
  distribution = "lognormal"
)

ml_lognormal

# Three-parameter Log-normal:  
ml_lognormal3 <- ml_estimation(
  alloy_tbl,
  distribution = "lognormal3"
)

ml_lognormal3
```

<br> 

```{r ML_visualization_I, fig.cap = "Figure 3: ML for a two-parametric log-normal distribution.", message = FALSE}
# Constructing probability plot: 
tbl_cdf_john <- estimate_cdf(alloy_tbl, "johnson")

lognormal_grid <- plot_prob(
  tbl_cdf_john,
  distribution = "lognormal", 
  title_main = "Log-normal Probability Plot", 
  title_x = "Cycles", 
  title_y = "Probability of Failure in %",
  title_trace = "Failed units",
  plot_method = "ggplot2"
)

# Add two-parametric model to grid: 
lognormal_plot <- plot_mod(
  lognormal_grid,
  x = ml_lognormal,
  title_trace = "Two-parametric log-normal"
)

lognormal_plot
```

<br> 

```{r ML_visualization_II, fig.cap = "Figure 4: ML for a three-parametric log-normal distribution.", message = FALSE}
# Add three-parametric model to lognormal_plot:
lognormal3_plot <- plot_mod(
  lognormal_grid, 
  x = ml_lognormal3, 
  title_trace = "Three-parametric log-normal"
)

lognormal3_plot
```

---

In this vignette two methods for the separation of mixture models are presented. 
A mixture model can be assumed, if the points in a probability plot show one or 
more changes in slope, depict one or several saddle points or follow an S-shape. 
A mixed distribution often represents the combination of multiple failure modes 
and thus must be split in its components to get reasonable results in further analyses. 

Segmented regression aims to detect breakpoints in the sample data from which a 
split in subgroups can be made. The expectation-maximization (EM) algorithm is a 
computation-intensive method that iteratively tries to maximize a likelihood function, 
which is weighted by posterior probabilities. These are conditional probabilities 
that an observation belongs to subgroup _k_.  

In the following, the focus is on the application of these methods and their 
visualizations using the functions `mixmod_regression()`, `mixmod_em()`, 
`plot_prob()` and `plot_mod()`. 

## Data: Voltage Stress Test

To apply the introduced methods the dataset `voltage` is used. The dataset contains 
observations for units that were passed to a high voltage stress test. _hours_ 
indicates the number of hours until a failure occurs or the number of hours until 
a unit was taken out of the test and has not failed. _status_ is a flag variable 
and describes the condition of a unit. If a unit has failed the flag is 1 and 0 
otherwise. The dataset is taken from _Reliability Analysis by Failure Mode_ [^note1]. 

[^note1]: Doganaksoy, N.; Hahn, G.; Meeker, W. Q.: _Reliability Analysis by Failure Mode_, 
          Quality Progress, 35(6), 47-52, 2002  
          
For consistent handling of the data, `weibulltools` introduces the function 
`reliability_data()` that converts the original dataset into a `wt_reliability_data` 
object. This formatted object allows to easily apply the presented methods.  

```{r dataset_voltage, message = FALSE}
voltage_tbl <- reliability_data(data = voltage, x = hours, status = status)
voltage_tbl
```

## Probability Plot for Voltage Stress Test Data

To get an intuition whether one can assume the presence of a mixture model, a 
Weibull probability plot is constructed. 

```{r probability_plot_weibull, fig.cap = "Figure 1: Plotting positions in Weibull grid.", message = FALSE}
# Estimating failure probabilities: 
voltage_cdf <- estimate_cdf(voltage_tbl, "johnson")

# Probability plot: 
weibull_plot <- plot_prob(
  voltage_cdf,
  distribution = "weibull", 
  title_main = "Weibull Probability Plot", 
  title_x = "Time in Hours", 
  title_y = "Probability of Failure in %",
  title_trace = "Defectives",
  plot_method = "ggplot2"
)

weibull_plot
```

<br>
Since there is one obvious slope change in the Weibull probability plot of _Figure 1_, 
the appearance of a mixture model consisting of two subgroups is strengthened.  

## Segmented Regression with Package `weibulltools`

The method of segmented regression is implemented in the function `mixmod_regression()`. 
If a breakpoint was detected, the failure data is separated by that point. After 
breakpoint detection the function `rank_regression()` is called inside `mixmod_regression()` 
and is used to estimate the distribution parameters of the subgroups. The visualization 
of the obtained results is done by functions `plot_prob()` and `plot_mod()`.  

```{r segmented_weibull_I, fig.cap = "Figure 2: Subgroup-specific plotting positions using segmented regression.", message = FALSE}
# Applying mixmod_regression(): 
mixreg_weib <- mixmod_regression(
  x = voltage_cdf, 
  distribution = "weibull", 
  k = 2
)

mixreg_weib

# Using plot_prob_mix(). 
mix_reg_plot <- plot_prob(
  x = mixreg_weib, 
  title_main = "Weibull Mixture Regression", 
  title_x = "Time in Hours", 
  title_y = "Probability of Failure", 
  title_trace = "Subgroup",
  plot_method = "ggplot2"
)

mix_reg_plot
```

<br>

```{r segmented_weibull_II, fig.cap = "Figure 3: Subgroup-specific regression lines using segmented regression.", message = FALSE}
# Using plot_mod() to visualize regression lines of subgroups: 
mix_reg_lines <- plot_mod(
  mix_reg_plot, 
  x = mixreg_weib, 
  title_trace = "Fitted Line"
)

mix_reg_lines
```

<br>
The method has separated the data into $k = 2$ subgroups. This can bee seen in 
_Figure 2_ and _Figure 3_.  
An upside of this function is that the segmentation is done in a comprehensible 
manner.  

Furthermore, the segmentation process can be done automatically by setting `k = NULL`. 
The danger here, however, is an overestimation of the breakpoints.  

To sum up, this function should give an intention of the existence of a mixture 
model. An in-depth analysis should be done afterwards.  

## EM Algorithm with Package `weibulltools`

The EM algorithm can be applied through the usage of the function `mixmod_em()`. 
In contrast to `mixmod_regression()`, this method does not support an automatic 
separation routine and therefore _k_, the number of subgroups, must always be specified.  

The obtained results can be also visualized by the functions `plot_prob()` and `plot_mod()`.  

```{r em_weibull_I, fig.cap = "Figure 4: Subgroup-specific plotting positions using EM algorithm.", message = FALSE}
# Applying mixmod_regression(): 
mix_em_weib <- mixmod_em(
  x = voltage_tbl, 
  distribution = "weibull",
  k = 2
)

mix_em_weib

# Using plot_prob(): 
mix_em_plot <- plot_prob(
  x = mix_em_weib,
  title_main = "Weibull Mixture EM", 
  title_x = "Time in Hours", 
  title_y = "Probability of Failure", 
  title_trace = "Subgroup",
  plot_method = "ggplot2"
)

mix_em_plot
```

```{r em_weibull_II, fig.cap = "Figure 5: Subgroup-specific regression lines using EM algorithm.", message = FALSE}

# Using plot_mod() to visualize regression lines of subgroups: 
mix_em_lines <- plot_mod(
  mix_em_plot, 
  x = mix_em_weib, 
  title_trace = "Fitted Line"
)

mix_em_lines
```

<br>
One advantage over `mixmod_regression()` is, that the EM algorithm can also assign 
censored items to a specific subgroup. Hence, an individual analysis of the mixing 
components, depicted in _Figure 4_ and _Figure 5_, is possible.  
In conclusion an analysis of a mixture model using `mixmod_em()` is statistically founded.

